{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenkao/miniconda3/envs/music-hw3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DataCollator, DatasetMIDI\n",
    "from torch.utils.data import DataLoader\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from midi_player import MIDIPlayer\n",
    "from model import ModelConfig, DatasetMIDI, PopTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig()\n",
    "tokenizer = REMI(TokenizerConfig())\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "midi_dir = Path(\"pop1k7/midi_analyzed\")\n",
    "\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=list(midi_dir.glob(\"**/*.mid\")),\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=config.max_seq_length,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size, collate_fn=collator)\n",
    "\n",
    "bos_token = tokenizer['BOS_None']\n",
    "eos_token = tokenizer['EOS_None']\n",
    "pad_token = tokenizer['PAD_None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD_None': 0,\n",
       " 'BOS_None': 1,\n",
       " 'EOS_None': 2,\n",
       " 'MASK_None': 3,\n",
       " 'Bar_None': 4,\n",
       " 'Pitch_21': 5,\n",
       " 'Pitch_22': 6,\n",
       " 'Pitch_23': 7,\n",
       " 'Pitch_24': 8,\n",
       " 'Pitch_25': 9,\n",
       " 'Pitch_26': 10,\n",
       " 'Pitch_27': 11,\n",
       " 'Pitch_28': 12,\n",
       " 'Pitch_29': 13,\n",
       " 'Pitch_30': 14,\n",
       " 'Pitch_31': 15,\n",
       " 'Pitch_32': 16,\n",
       " 'Pitch_33': 17,\n",
       " 'Pitch_34': 18,\n",
       " 'Pitch_35': 19,\n",
       " 'Pitch_36': 20,\n",
       " 'Pitch_37': 21,\n",
       " 'Pitch_38': 22,\n",
       " 'Pitch_39': 23,\n",
       " 'Pitch_40': 24,\n",
       " 'Pitch_41': 25,\n",
       " 'Pitch_42': 26,\n",
       " 'Pitch_43': 27,\n",
       " 'Pitch_44': 28,\n",
       " 'Pitch_45': 29,\n",
       " 'Pitch_46': 30,\n",
       " 'Pitch_47': 31,\n",
       " 'Pitch_48': 32,\n",
       " 'Pitch_49': 33,\n",
       " 'Pitch_50': 34,\n",
       " 'Pitch_51': 35,\n",
       " 'Pitch_52': 36,\n",
       " 'Pitch_53': 37,\n",
       " 'Pitch_54': 38,\n",
       " 'Pitch_55': 39,\n",
       " 'Pitch_56': 40,\n",
       " 'Pitch_57': 41,\n",
       " 'Pitch_58': 42,\n",
       " 'Pitch_59': 43,\n",
       " 'Pitch_60': 44,\n",
       " 'Pitch_61': 45,\n",
       " 'Pitch_62': 46,\n",
       " 'Pitch_63': 47,\n",
       " 'Pitch_64': 48,\n",
       " 'Pitch_65': 49,\n",
       " 'Pitch_66': 50,\n",
       " 'Pitch_67': 51,\n",
       " 'Pitch_68': 52,\n",
       " 'Pitch_69': 53,\n",
       " 'Pitch_70': 54,\n",
       " 'Pitch_71': 55,\n",
       " 'Pitch_72': 56,\n",
       " 'Pitch_73': 57,\n",
       " 'Pitch_74': 58,\n",
       " 'Pitch_75': 59,\n",
       " 'Pitch_76': 60,\n",
       " 'Pitch_77': 61,\n",
       " 'Pitch_78': 62,\n",
       " 'Pitch_79': 63,\n",
       " 'Pitch_80': 64,\n",
       " 'Pitch_81': 65,\n",
       " 'Pitch_82': 66,\n",
       " 'Pitch_83': 67,\n",
       " 'Pitch_84': 68,\n",
       " 'Pitch_85': 69,\n",
       " 'Pitch_86': 70,\n",
       " 'Pitch_87': 71,\n",
       " 'Pitch_88': 72,\n",
       " 'Pitch_89': 73,\n",
       " 'Pitch_90': 74,\n",
       " 'Pitch_91': 75,\n",
       " 'Pitch_92': 76,\n",
       " 'Pitch_93': 77,\n",
       " 'Pitch_94': 78,\n",
       " 'Pitch_95': 79,\n",
       " 'Pitch_96': 80,\n",
       " 'Pitch_97': 81,\n",
       " 'Pitch_98': 82,\n",
       " 'Pitch_99': 83,\n",
       " 'Pitch_100': 84,\n",
       " 'Pitch_101': 85,\n",
       " 'Pitch_102': 86,\n",
       " 'Pitch_103': 87,\n",
       " 'Pitch_104': 88,\n",
       " 'Pitch_105': 89,\n",
       " 'Pitch_106': 90,\n",
       " 'Pitch_107': 91,\n",
       " 'Pitch_108': 92,\n",
       " 'Velocity_3': 93,\n",
       " 'Velocity_7': 94,\n",
       " 'Velocity_11': 95,\n",
       " 'Velocity_15': 96,\n",
       " 'Velocity_19': 97,\n",
       " 'Velocity_23': 98,\n",
       " 'Velocity_27': 99,\n",
       " 'Velocity_31': 100,\n",
       " 'Velocity_35': 101,\n",
       " 'Velocity_39': 102,\n",
       " 'Velocity_43': 103,\n",
       " 'Velocity_47': 104,\n",
       " 'Velocity_51': 105,\n",
       " 'Velocity_55': 106,\n",
       " 'Velocity_59': 107,\n",
       " 'Velocity_63': 108,\n",
       " 'Velocity_67': 109,\n",
       " 'Velocity_71': 110,\n",
       " 'Velocity_75': 111,\n",
       " 'Velocity_79': 112,\n",
       " 'Velocity_83': 113,\n",
       " 'Velocity_87': 114,\n",
       " 'Velocity_91': 115,\n",
       " 'Velocity_95': 116,\n",
       " 'Velocity_99': 117,\n",
       " 'Velocity_103': 118,\n",
       " 'Velocity_107': 119,\n",
       " 'Velocity_111': 120,\n",
       " 'Velocity_115': 121,\n",
       " 'Velocity_119': 122,\n",
       " 'Velocity_123': 123,\n",
       " 'Velocity_127': 124,\n",
       " 'Duration_0.1.8': 125,\n",
       " 'Duration_0.2.8': 126,\n",
       " 'Duration_0.3.8': 127,\n",
       " 'Duration_0.4.8': 128,\n",
       " 'Duration_0.5.8': 129,\n",
       " 'Duration_0.6.8': 130,\n",
       " 'Duration_0.7.8': 131,\n",
       " 'Duration_1.0.8': 132,\n",
       " 'Duration_1.1.8': 133,\n",
       " 'Duration_1.2.8': 134,\n",
       " 'Duration_1.3.8': 135,\n",
       " 'Duration_1.4.8': 136,\n",
       " 'Duration_1.5.8': 137,\n",
       " 'Duration_1.6.8': 138,\n",
       " 'Duration_1.7.8': 139,\n",
       " 'Duration_2.0.8': 140,\n",
       " 'Duration_2.1.8': 141,\n",
       " 'Duration_2.2.8': 142,\n",
       " 'Duration_2.3.8': 143,\n",
       " 'Duration_2.4.8': 144,\n",
       " 'Duration_2.5.8': 145,\n",
       " 'Duration_2.6.8': 146,\n",
       " 'Duration_2.7.8': 147,\n",
       " 'Duration_3.0.8': 148,\n",
       " 'Duration_3.1.8': 149,\n",
       " 'Duration_3.2.8': 150,\n",
       " 'Duration_3.3.8': 151,\n",
       " 'Duration_3.4.8': 152,\n",
       " 'Duration_3.5.8': 153,\n",
       " 'Duration_3.6.8': 154,\n",
       " 'Duration_3.7.8': 155,\n",
       " 'Duration_4.0.4': 156,\n",
       " 'Duration_4.1.4': 157,\n",
       " 'Duration_4.2.4': 158,\n",
       " 'Duration_4.3.4': 159,\n",
       " 'Duration_5.0.4': 160,\n",
       " 'Duration_5.1.4': 161,\n",
       " 'Duration_5.2.4': 162,\n",
       " 'Duration_5.3.4': 163,\n",
       " 'Duration_6.0.4': 164,\n",
       " 'Duration_6.1.4': 165,\n",
       " 'Duration_6.2.4': 166,\n",
       " 'Duration_6.3.4': 167,\n",
       " 'Duration_7.0.4': 168,\n",
       " 'Duration_7.1.4': 169,\n",
       " 'Duration_7.2.4': 170,\n",
       " 'Duration_7.3.4': 171,\n",
       " 'Duration_8.0.4': 172,\n",
       " 'Duration_8.1.4': 173,\n",
       " 'Duration_8.2.4': 174,\n",
       " 'Duration_8.3.4': 175,\n",
       " 'Duration_9.0.4': 176,\n",
       " 'Duration_9.1.4': 177,\n",
       " 'Duration_9.2.4': 178,\n",
       " 'Duration_9.3.4': 179,\n",
       " 'Duration_10.0.4': 180,\n",
       " 'Duration_10.1.4': 181,\n",
       " 'Duration_10.2.4': 182,\n",
       " 'Duration_10.3.4': 183,\n",
       " 'Duration_11.0.4': 184,\n",
       " 'Duration_11.1.4': 185,\n",
       " 'Duration_11.2.4': 186,\n",
       " 'Duration_11.3.4': 187,\n",
       " 'Duration_12.0.4': 188,\n",
       " 'Position_0': 189,\n",
       " 'Position_1': 190,\n",
       " 'Position_2': 191,\n",
       " 'Position_3': 192,\n",
       " 'Position_4': 193,\n",
       " 'Position_5': 194,\n",
       " 'Position_6': 195,\n",
       " 'Position_7': 196,\n",
       " 'Position_8': 197,\n",
       " 'Position_9': 198,\n",
       " 'Position_10': 199,\n",
       " 'Position_11': 200,\n",
       " 'Position_12': 201,\n",
       " 'Position_13': 202,\n",
       " 'Position_14': 203,\n",
       " 'Position_15': 204,\n",
       " 'Position_16': 205,\n",
       " 'Position_17': 206,\n",
       " 'Position_18': 207,\n",
       " 'Position_19': 208,\n",
       " 'Position_20': 209,\n",
       " 'Position_21': 210,\n",
       " 'Position_22': 211,\n",
       " 'Position_23': 212,\n",
       " 'Position_24': 213,\n",
       " 'Position_25': 214,\n",
       " 'Position_26': 215,\n",
       " 'Position_27': 216,\n",
       " 'Position_28': 217,\n",
       " 'Position_29': 218,\n",
       " 'Position_30': 219,\n",
       " 'Position_31': 220,\n",
       " 'PitchDrum_27': 221,\n",
       " 'PitchDrum_28': 222,\n",
       " 'PitchDrum_29': 223,\n",
       " 'PitchDrum_30': 224,\n",
       " 'PitchDrum_31': 225,\n",
       " 'PitchDrum_32': 226,\n",
       " 'PitchDrum_33': 227,\n",
       " 'PitchDrum_34': 228,\n",
       " 'PitchDrum_35': 229,\n",
       " 'PitchDrum_36': 230,\n",
       " 'PitchDrum_37': 231,\n",
       " 'PitchDrum_38': 232,\n",
       " 'PitchDrum_39': 233,\n",
       " 'PitchDrum_40': 234,\n",
       " 'PitchDrum_41': 235,\n",
       " 'PitchDrum_42': 236,\n",
       " 'PitchDrum_43': 237,\n",
       " 'PitchDrum_44': 238,\n",
       " 'PitchDrum_45': 239,\n",
       " 'PitchDrum_46': 240,\n",
       " 'PitchDrum_47': 241,\n",
       " 'PitchDrum_48': 242,\n",
       " 'PitchDrum_49': 243,\n",
       " 'PitchDrum_50': 244,\n",
       " 'PitchDrum_51': 245,\n",
       " 'PitchDrum_52': 246,\n",
       " 'PitchDrum_53': 247,\n",
       " 'PitchDrum_54': 248,\n",
       " 'PitchDrum_55': 249,\n",
       " 'PitchDrum_56': 250,\n",
       " 'PitchDrum_57': 251,\n",
       " 'PitchDrum_58': 252,\n",
       " 'PitchDrum_59': 253,\n",
       " 'PitchDrum_60': 254,\n",
       " 'PitchDrum_61': 255,\n",
       " 'PitchDrum_62': 256,\n",
       " 'PitchDrum_63': 257,\n",
       " 'PitchDrum_64': 258,\n",
       " 'PitchDrum_65': 259,\n",
       " 'PitchDrum_66': 260,\n",
       " 'PitchDrum_67': 261,\n",
       " 'PitchDrum_68': 262,\n",
       " 'PitchDrum_69': 263,\n",
       " 'PitchDrum_70': 264,\n",
       " 'PitchDrum_71': 265,\n",
       " 'PitchDrum_72': 266,\n",
       " 'PitchDrum_73': 267,\n",
       " 'PitchDrum_74': 268,\n",
       " 'PitchDrum_75': 269,\n",
       " 'PitchDrum_76': 270,\n",
       " 'PitchDrum_77': 271,\n",
       " 'PitchDrum_78': 272,\n",
       " 'PitchDrum_79': 273,\n",
       " 'PitchDrum_80': 274,\n",
       " 'PitchDrum_81': 275,\n",
       " 'PitchDrum_82': 276,\n",
       " 'PitchDrum_83': 277,\n",
       " 'PitchDrum_84': 278,\n",
       " 'PitchDrum_85': 279,\n",
       " 'PitchDrum_86': 280,\n",
       " 'PitchDrum_87': 281}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pitch_21', 5),\n",
       " ('Pitch_22', 6),\n",
       " ('Pitch_23', 7),\n",
       " ('Pitch_24', 8),\n",
       " ('Pitch_25', 9),\n",
       " ('Pitch_26', 10),\n",
       " ('Pitch_27', 11),\n",
       " ('Pitch_28', 12),\n",
       " ('Pitch_29', 13),\n",
       " ('Pitch_30', 14),\n",
       " ('Pitch_31', 15),\n",
       " ('Pitch_32', 16),\n",
       " ('Pitch_33', 17),\n",
       " ('Pitch_34', 18),\n",
       " ('Pitch_35', 19),\n",
       " ('Pitch_36', 20),\n",
       " ('Pitch_37', 21),\n",
       " ('Pitch_38', 22),\n",
       " ('Pitch_39', 23),\n",
       " ('Pitch_40', 24),\n",
       " ('Pitch_41', 25),\n",
       " ('Pitch_42', 26),\n",
       " ('Pitch_43', 27),\n",
       " ('Pitch_44', 28),\n",
       " ('Pitch_45', 29),\n",
       " ('Pitch_46', 30),\n",
       " ('Pitch_47', 31),\n",
       " ('Pitch_48', 32),\n",
       " ('Pitch_49', 33),\n",
       " ('Pitch_50', 34),\n",
       " ('Pitch_51', 35),\n",
       " ('Pitch_52', 36),\n",
       " ('Pitch_53', 37),\n",
       " ('Pitch_54', 38),\n",
       " ('Pitch_55', 39),\n",
       " ('Pitch_56', 40),\n",
       " ('Pitch_57', 41),\n",
       " ('Pitch_58', 42),\n",
       " ('Pitch_59', 43),\n",
       " ('Pitch_60', 44),\n",
       " ('Pitch_61', 45),\n",
       " ('Pitch_62', 46),\n",
       " ('Pitch_63', 47),\n",
       " ('Pitch_64', 48),\n",
       " ('Pitch_65', 49),\n",
       " ('Pitch_66', 50),\n",
       " ('Pitch_67', 51),\n",
       " ('Pitch_68', 52),\n",
       " ('Pitch_69', 53),\n",
       " ('Pitch_70', 54),\n",
       " ('Pitch_71', 55),\n",
       " ('Pitch_72', 56),\n",
       " ('Pitch_73', 57),\n",
       " ('Pitch_74', 58),\n",
       " ('Pitch_75', 59),\n",
       " ('Pitch_76', 60),\n",
       " ('Pitch_77', 61),\n",
       " ('Pitch_78', 62),\n",
       " ('Pitch_79', 63),\n",
       " ('Pitch_80', 64),\n",
       " ('Pitch_81', 65),\n",
       " ('Pitch_82', 66),\n",
       " ('Pitch_83', 67),\n",
       " ('Pitch_84', 68),\n",
       " ('Pitch_85', 69),\n",
       " ('Pitch_86', 70),\n",
       " ('Pitch_87', 71),\n",
       " ('Pitch_88', 72),\n",
       " ('Pitch_89', 73),\n",
       " ('Pitch_90', 74),\n",
       " ('Pitch_91', 75),\n",
       " ('Pitch_92', 76),\n",
       " ('Pitch_93', 77),\n",
       " ('Pitch_94', 78),\n",
       " ('Pitch_95', 79),\n",
       " ('Pitch_96', 80),\n",
       " ('Pitch_97', 81),\n",
       " ('Pitch_98', 82),\n",
       " ('Pitch_99', 83),\n",
       " ('Pitch_100', 84),\n",
       " ('Pitch_101', 85),\n",
       " ('Pitch_102', 86),\n",
       " ('Pitch_103', 87),\n",
       " ('Pitch_104', 88),\n",
       " ('Pitch_105', 89),\n",
       " ('Pitch_106', 90),\n",
       " ('Pitch_107', 91),\n",
       " ('Pitch_108', 92)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v) for k, v in tokenizer.vocab.items() if \"Pitch_\" in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(tokenizer, model, max_len=2000):\n",
    "    sample_size = 1\n",
    "    bos_token = tokenizer['BOS_None']\n",
    "    eos_token = tokenizer['EOS_None']\n",
    "    pad_token = tokenizer['PAD_None']\n",
    "    tokens = torch.tensor([bos_token]).reshape(sample_size, 1) # (batch_n, seq_len)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        padding_mask = torch.tensor([pad_token] * tokens.shape[1]).reshape(sample_size, -1)\n",
    "        logits = model(tokens, padding_mask) # (batch_n, seq_len, vocab_size)\n",
    "        prob = torch.nn.functional.softmax(logits, dim=2)\n",
    "        next_token = torch.multinomial(prob[:, -1], 1) # (batch_n, 1)\n",
    "        tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        if next_token[0, -1] == eos_token:\n",
    "            break\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# tokens = sample(tokenizer, model)\n",
    "# len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenkao/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PopTransformer(\n",
    "    vocab_size=config.vocab_size,\n",
    "    d_model=config.d_model,\n",
    "    nhead=config.n_head,\n",
    "    num_encoder_layers=config.num_encoder_layers,\n",
    "    dim_feedforward=config.dim_feedforward,\n",
    "    max_seq_length=config.max_seq_length\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "ctriterion = nn.CrossEntropyLoss(ignore_index=tokenizer[\"PAD_None\"])\n",
    "data = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30021333694458\n",
      "1.7786098718643188\n",
      "0.6422502994537354\n",
      "0.24322421848773956\n",
      "0.11996977031230927\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    tokens = data[\"input_ids\"]\n",
    "    mask = data[\"attention_mask\"]\n",
    "\n",
    "    logits = model(tokens, mask)\n",
    "    n_batch, n_seq, vocab_size = logits.shape\n",
    "    logits = logits.view(n_batch * n_seq, vocab_size)\n",
    "    tokens = tokens.flatten()\n",
    "    loss = ctriterion(logits, tokens)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;script src=&quot;https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0&quot;&gt;&lt;/script&gt;  &amp;nbsp; &lt;a href=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQABAAhNVHJrAAAAEwD/WAQEAhgIAP9RAwehIAH/LwA=&quot; target=&quot;_blank&quot;&gt;Download MIDI&lt;/a&gt;&lt;br&gt;\n",
       "            &lt;midi-player src=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQABAAhNVHJrAAAAEwD/WAQEAhgIAP9RAwehIAH/LwA=&quot; sound-font visualizer=&quot;#myVisualizer&quot;&gt;&lt;/midi-player&gt;\n",
       "            &lt;midi-visualizer type=&quot;piano-roll&quot; id=&quot;myVisualizer&quot; style=&quot;background: #fff;&quot;&gt;&lt;/midi-visualizer&gt;\" width=\"100%\" height=\"100\"\n",
       "            style=\"border:none !important;\"\n",
       "            \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'\n",
       "            </iframe>"
      ],
      "text/plain": [
       "<midi_player.midi_player.MIDIPlayer at 0x113c40af0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sample(tokenizer, model)\n",
    "print(tokens.shape)\n",
    "\n",
    "score = tokenizer.decode(tokens)\n",
    "score.dump_midi(\"sample.mid\")\n",
    "MIDIPlayer(\"/Users/stevenkao/workspace/music-hw-3/sample.mid\", 100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))\n",
    "tokens = data['input_ids']\n",
    "\n",
    "tokens = tokens[0:1, :]\n",
    "score = tokenizer.decode(tokens)\n",
    "score.dump_midi(\"sample.mid\")\n",
    "\n",
    "MIDIPlayer(\"/Users/stevenkao/workspace/music-hw-3/sample.mid\", 400)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "bos_token = tokenizer['BOS_None']\n",
    "eos_token = tokenizer['EOS_None']\n",
    "pad_token = tokenizer['PAD_None']\n",
    "\n",
    "gpt_config = GPT2Config(\n",
    "    vocab_size=config.vocab_size,\n",
    "    n_positions=config.max_seq_length,\n",
    "    n_embd=config.d_model,\n",
    "    n_layer=config.num_encoder_layers,\n",
    "    n_head=config.n_head,\n",
    "    bos_token_id=bos_token,\n",
    "    eos_token_id=eos_token,\n",
    ")\n",
    "\n",
    "gpt2 = GPT2LMHeadModel(gpt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 511, 282])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = next(iter(dataloader))\n",
    "tokens = data['input_ids']\n",
    "attention_mask = data['attention_mask']\n",
    "tokens.shape\n",
    "\n",
    "out = gpt2(tokens, attention_mask=attention_mask)\n",
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6560187339782715\n",
      "1.5040385723114014\n",
      "0.3334512412548065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m tokens \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     14\u001b[0m n_batch, n_seq, vocab_size \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1132\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1121\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1122\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         output_attentions,\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:615\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    613\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    614\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 615\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    624\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/music-hw3/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:545\u001b[0m, in \u001b[0;36mGPT2SdpaAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_dropout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Reshape outputs\u001b[39;00m\n\u001b[1;32m    555\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(gpt2.parameters())\n",
    "ctriterion = nn.CrossEntropyLoss(ignore_index=tokenizer[\"PAD_None\"])\n",
    "data = next(iter(dataloader))\n",
    "\n",
    "for i in range(100):\n",
    "    gpt2.train()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    tokens = data[\"input_ids\"]\n",
    "    mask = data[\"attention_mask\"]\n",
    "    out = gpt2(tokens, attention_mask=mask)\n",
    "    logits = out.logits\n",
    "\n",
    "    n_batch, n_seq, vocab_size = logits.shape\n",
    "    logits = logits.view(n_batch * n_seq, vocab_size)\n",
    "    tokens = tokens.flatten()\n",
    "    loss = ctriterion(logits, tokens)\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   1,   1,   1,   1,   1, 130, 130, 130, 130, 130, 130, 130,  46,\n",
       "          46,  46,  46,  46,  46,  46,  46,  46, 160, 160,  43,  43,  43,  43,\n",
       "          43,  43,  43,  43,  43, 197, 197, 197, 197, 105, 104,  38,  38,  38,\n",
       "          38,  38,  38, 105, 105, 105, 105, 105, 105, 105, 105, 143, 143, 143,\n",
       "         143, 143, 143, 143, 145, 145, 145, 145, 145, 145, 145, 145, 145, 145,\n",
       "         145,  54,  54,  54,  54,  54,  54,  54,  54,  54,  54,  54,  54,  54,\n",
       "          54,  54,  54,  54,  54,  54,  54,  54,  54,  54,  54, 165, 165, 165,\n",
       "         165, 165]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 1\n",
    "bos_token = tokenizer['BOS_None']\n",
    "eos_token = tokenizer['EOS_None']\n",
    "pad_token = tokenizer['PAD_None']\n",
    "tokens = torch.tensor([bos_token]).reshape(sample_size, 1) # (batch_n, seq_len)\n",
    "\n",
    "sample = gpt2.generate(tokens, do_sample=True, max_length=100)\n",
    "sample\n",
    "    # for _ in range(max_len):\n",
    "    #     padding_mask = torch.tensor([pad_token] * tokens.shape[1]).reshape(sample_size, -1)\n",
    "    #     logits = model(tokens, padding_mask) # (batch_n, seq_len, vocab_size)\n",
    "    #     prob = torch.nn.functional.softmax(logits, dim=2)\n",
    "    #     next_token = torch.multinomial(prob[:, -1], 1) # (batch_n, 1)\n",
    "    #     tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "    #     if next_token[0, -1] == eos_token:\n",
    "    #         break\n",
    "\n",
    "    # return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mvocab\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
